{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "\n",
    "This lab consists of 2 parts:\n",
    " 1. Derivation of stochastic gradient descent update for logistic regression\n",
    " 2. Implementation of perceptron with a non-linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theory, gradient descent for logistic regression\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a binary **classification** technique, where the classes are 0 and 1. We will define our classifier as $h\\left(x\\right) = \\sigma \\left(w^T x + b\\right)$, and we will interpret $h\\left(x\\right)$ as being the probability of obtaining class 1, $p\\left( y=1|x \\right)$. Consequently, the probability of obtaining class $0$ is simply $1-h\\left(x\\right)$.\n",
    "\n",
    "For logistic regression we will use cross entropy as our cost function: If the real class is $y \\in \\left\\{ 0, 1 \\right\\}$ then the error for an example is $\\ell \\left(x, y\\right) = - \\log p\\left(y|x,h\\left(x\\right)\\right)$. More precisely,\n",
    " - If $y=1$ then $\\ell \\left(h\\left(x\\right), 1\\right) = - \\log p\\left(1|x,h\\left(x\\right)\\right) = - \\log \\left( h\\left(x\\right) \\right)$\n",
    " - If $y=0$ then $\\ell \\left(h\\left(x\\right), 0\\right) = - \\log p\\left(0|x,h\\left(x\\right)\\right) = - \\log \\left( 1 - h\\left(x\\right)\\right)$\n",
    " \n",
    "This can be rewritten more compactly:\n",
    "\n",
    "$\\ell \\left(h\\left(x\\right), y\\right) = - y \\log \\left( h\\left(x\\right) \\right) - \\left( 1 - y \\right) \\log \\left( 1 - h\\left(x\\right)\\right)$\n",
    "\n",
    "### Learning by stochastic gradient descent\n",
    "\n",
    "Now that we have a model and a cost function, we will want to learn the parameters of the model by using stochastic gradient descent. Like last week, this consists in iteratively modifying the parameters by looping on the examples $\\left(x, y\\right)$ and by adding $- \\mu \\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial \\theta}$. $\\mu$ is the learning rate, and $\\theta$ is the set of parameters, which in our case are $w$ and $b$.\n",
    "\n",
    "The first step is to determine the gradient $\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial \\theta}$. For this, we will use the chain rule for derivation. In a few sessions, we will also use it for the backpropagation in neural networks. This gives: \n",
    "\n",
    "$$\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial w} = \\frac{\\partial \\ell \\left(h, y\\right)}{\\partial h} \\frac{\\partial h\\left(u\\right)}{\\partial u} \\frac{\\partial u\\left(x, w, b\\right)}{\\partial w}$$\n",
    "\n",
    "$$\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial b} = \\frac{\\partial \\ell \\left(h, y\\right)}{\\partial h} \\frac{\\partial h\\left(u\\right)}{\\partial u} \\frac{\\partial u\\left( x, w, b\\right)}{\\partial b}$$\n",
    "\n",
    "We note $u\\left(x, w, b\\right) = w^T x + b$.\n",
    "\n",
    "**Exercice: We ask you to calculate the gradients $\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial w}$ and $\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial b}$.**\n",
    "\n",
    " - $\\frac{\\partial\\ell\\left(h,y\\right)}{\\partial h}=\\frac{\\partial}{\\partial h}\\left\\{ y\\log\\left(h\\right)-\\left(1-y\\right)\\log\\left(1-h\\right)\\right\\} =-\\frac{y}{h}+\\frac{1-y}{1-h}$\n",
    " - $\\frac{\\partial h\\left(u\\right)}{\\partial u}=\\frac{\\partial\\sigma\\left(u\\right)}{\\partial u}=\\sigma\\left(u\\right)\\left(1-\\sigma\\left(u\\right)\\right)$\n",
    " - $\\frac{\\partial u\\left(x,w,b\\right)}{\\partial w}=\\frac{\\partial}{\\partial w}\\left\\{ w^{T}x+b\\right\\} =x^{T}$\n",
    " - $\\frac{\\partial u\\left(x,w,b\\right)}{\\partial b}=\\frac{\\partial}{\\partial w}\\left\\{ w^{T}x+b\\right\\} =1$\n",
    "\n",
    "We then take the product of the partial derivatives, and we notice that it simplifies to:\n",
    " - $\\frac{\\partial\\ell\\left(h,y\\right)}{\\partial h}\\frac{\\partial h\\left(u\\right)}{\\partial u}=\\left(-\\frac{y}{h\\left(u\\right)}+\\frac{1-y}{1-h\\left(u\\right)}\\right)h\\left(u\\right)\\left(1-h\\left(u\\right)\\right)=y\\left(h\\left(u\\right)-1\\right)+\\left(1-y\\right)h\\left(u\\right)=h\\left(u\\right)-y$\n",
    " \n",
    "Finally, we obtain:\n",
    " - $\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial w} = \\left(h\\left(u\\right)-y\\right) x^T$\n",
    " - $\\frac{\\partial \\ell \\left(h\\left(x\\right), y\\right)}{\\partial b} = h\\left(u\\right)-y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Practice, Non-linear transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import utilities\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the use of non-linear transformations, we will make use of the circle and ellipse datasets, available on the website. We will first apply a transformation on the dataset before training. The algorithm will then uniquely work with the transformed data. To keep things simple at first, we will simply implement a transformation consisting of a degree two polynomial.\n",
    "\n",
    "Reminder:\n",
    "\n",
    "$\\phi_{\\text{poly}^2}(x) = \\left(x_1,x_2,\\dots,x_d,\n",
    "\\alpha_{11} x_1^2, \\alpha_{22}x2^2, \\dots, \\alpha_{dd}x_d^2,\n",
    "\\alpha_{12}x_1x_2, \\alpha_{13}x_1x_2, \\dots, \\alpha_{1d}x_1x_d, \\dots, \\alpha_{(d-1)d}x_{d-1}x_d\\right)$\n",
    "\n",
    "To simplify, we can use $\\alpha_i=1$ $\\forall i$\n",
    "\n",
    "And to make it even more simple, the circle and ellipse datasets are only in 2-d...\n",
    "\n",
    "$$\\phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$$\n",
    "\n",
    "$$\\phi(x_1, x_2) = \\left(1,x_1,x_2,x_1^2,x_2^2,x_1x_2\\right)$$\n",
    "\n",
    "**Exercise: implement this transformation in the polynomial function below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a matrix of data examples x (n datapoints, input space dimension) as input (without labels) and returns the transformed\n",
    "# matrix (n datapoints, projected space dimension)\n",
    "def polynomial(X):\n",
    "    Y = np.zeros((X.shape[0], 6))\n",
    "    Y[:,0] = 1.\n",
    "    Y[:,1] = X[:,0]\n",
    "    Y[:,2] = X[:,1]\n",
    "    Y[:,3] = X[:,0]**2\n",
    "    Y[:,4] = X[:,1]**2\n",
    "    Y[:,5] = X[:,0]*X[:,1]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the transformation is implemented, you can run the code below to look at the performance of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self, mu):\n",
    "        self.mu = mu\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        n_example = train_data.shape[0]\n",
    "            \n",
    "        self.weights = np.random.random(train_data.shape[1]-1)\n",
    "\n",
    "        i = 0\n",
    "        count = 0 # We stop when the set is linearly separated\n",
    "        n_iter = 0\n",
    "        n_iter_max = n_example*100\n",
    "        while (count < n_example and n_iter < n_iter_max):\n",
    "            if (np.dot(train_data[i, :-1], self.weights)) * train_data[i,-1] < 0:\n",
    "                self.weights += self.mu * train_data[i,-1] * train_data[i, :-1]\n",
    "                count = 0\n",
    "            else:\n",
    "                  count = count + 1\n",
    "            i = (i + 1) % n_example\n",
    "            n_iter += 1\n",
    "\n",
    "    def compute_predictions(self, test_data):\n",
    "        outputs = []\n",
    "        for i in range(len(test_data)):\n",
    "            data = []\n",
    "            for j in range(len(test_data[i])):\n",
    "                data.append(test_data[i][j])\n",
    "            outputs.append(np.dot(data, self.weights))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading the dataset\n",
    "data = np.loadtxt('cercle.txt')\n",
    "#data = np.loadtxt('ellipse.txt')\n",
    "\n",
    "# There are only 2 dimensions...\n",
    "train_cols = [0,1]\n",
    "# A variable to hold the index of the label column.\n",
    "target_ind = [data.shape[1] - 1]\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 2\n",
    "# Number of training points\n",
    "n_train = 1500\n",
    "# Grid size = grid_size x grid_size\n",
    "grid_size = 50\n",
    "\n",
    "print(\"We will train a perceptron on \", n_train, \" training examples\")\n",
    "\n",
    "# Uncomment to get non-deterministic results \n",
    "random.seed(3395)\n",
    "\n",
    "# Randomly select indices for the training and test set\n",
    "inds = np.arange(data.shape[0])\n",
    "random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "    \n",
    "# Separate the dataset into two sets: training and test.\n",
    "train_set = data[train_inds,:]\t# keep the good rows\n",
    "train_set = train_set[:,train_cols + target_ind]  # keep the right columns\n",
    "test_set = data[test_inds,:]\n",
    "test_set = test_set[:,train_cols + target_ind]\n",
    "\n",
    "# Separate the test set: inputs and labels.\n",
    "test_inputs = test_set[:,:-1]\n",
    "test_labels = test_set[:,-1]\n",
    "\n",
    "# The learning rate\n",
    "mu = 0.005\n",
    "\n",
    "# Transform the dataset\n",
    "transformed_train_set = np.concatenate((polynomial(train_set[:,:-1]),train_set[:,-1][:,None]),axis=1)\n",
    "transformed_test_inputs = polynomial(test_inputs)\n",
    "\n",
    "# Create and train the model\n",
    "model_perceptron = perceptron(mu)\n",
    "model_perceptron.train(transformed_train_set)\n",
    "\n",
    "# Obtain the ouputs on the test set.\n",
    "t1 = time.clock()\n",
    "outputs = model_perceptron.compute_predictions(transformed_test_inputs)\n",
    "t2 = time.clock()\n",
    "print('It took us ', t2-t1, ' seconds to calculate the predictions on ', transformed_test_inputs.shape[0],' test datapoints')\n",
    "\n",
    "# Convert the outputs into classes by taking the sign.\n",
    "classes_pred = np.sign(outputs)\n",
    "   \n",
    "# Measure the performance\n",
    "err = 1.0 - np.mean(test_labels==classes_pred)\n",
    "print(\"The test error is \", 100.0 * err,\"%\")\n",
    "\n",
    "# Plotting a graph\n",
    "if len(train_cols) == 2:\n",
    "    # Decision boundary\n",
    "    t1 = time.clock()\n",
    "    outputs = model_perceptron.compute_predictions(transformed_train_set[:,:-1])\n",
    "    # Convert the outputs into classes by taking the sign.\n",
    "    train_classes_pred = np.sign(outputs)\n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_classes_pred)\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=classes_pred)\n",
    "    plt.title('Prediction')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_set[:,-1])\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=test_labels)\n",
    "    plt.title('True labels')\n",
    "    plt.show()\n",
    "\n",
    "    t2 = time.clock()\n",
    "        \n",
    "else:\n",
    "    print('Too many dimensions (', len(train_cols),') to draw the surface decision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel trick\n",
    "\n",
    "Your objective for this section is to implement a gaussian kernel perceptron. We will ask you to use a gaussian kernel for this problem. For a review, please refer to the [course document](https://studium.umontreal.ca/pluginfile.php/4043631/mod_resource/content/2/13_kernel_trick-en-3395.pdf), especially the second part.\n",
    "\n",
    "**Exercise: implement a polynomial kernel function of arbitrary degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_polynomial(x,y,deg=2):\n",
    "    return 1 + np.dot(x, y.T)**deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the kernel perceptron.\n",
    "\n",
    "**Exercise: Complete the code below for the kernel perceptron algorithmn**\n",
    "\n",
    "Hint: Reuse the perceptron code above, but rather than learning the vector $w$ directly, you need to rewrite $w = \\sum_i \\alpha_i x_i$ and to learn the coefficients $a_i$. Then, we replace $w^T x$ by $\\left(\\sum_i \\alpha_i x_i\\right)^T x$ and apply the kernel trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, mu, kernel_fn):\n",
    "        self.mu = mu\n",
    "        self.kernel_fn = kernel_fn\n",
    "\n",
    "    def train(self, train_data):\n",
    "        n_example = train_data.shape[0]\n",
    "        self.train_data = np.array(train_data)\n",
    "\n",
    "        # alpha initialisation\n",
    "        self.a = np.zeros(n_example)\n",
    "\n",
    "        # Get rid of the labels\n",
    "        train_x = self.train_x = self.train_data[:,:-1]\n",
    "        train_y = self.train_y = self.train_data[:,-1]\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_example, n_example))\n",
    "        for i in range(n_example):\n",
    "            K[i] = self.kernel_fn(train_x[i], train_x)\n",
    "\n",
    "        # Kernel calculation\n",
    "        i = 0\n",
    "        count = 0\n",
    "        n_iter = 0\n",
    "        n_iter_max = n_example*100\n",
    "        while (count < n_example and n_iter < n_iter_max):\n",
    "            if np.sign(np.sum(K[i] * self.a * train_y)) != train_y[i]:\n",
    "                self.a[i] += 1.0\n",
    "                count = 0\n",
    "            else:\n",
    "                count = count + 1\n",
    "            i = (i + 1) % n_example\n",
    "            n_iter += 1\n",
    "\n",
    "    def compute_predictions(self, test_data):\n",
    "        outputs = []\n",
    "        for i in range(len(test_data)):\n",
    "            outputs.append(np.sum(self.kernel_fn(test_data[i], self.train_x)*self.a*self.train_y))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "#data = np.loadtxt('ellipse.txt')\n",
    "data = np.loadtxt('cercle.txt')\n",
    "\n",
    "# The columns (dim/features) on which we will learn our model\n",
    "# For gridplot to work, len(train_cols) should be 2\n",
    "train_cols = [0,1]\n",
    "# Index of the column containing the labels\n",
    "target_ind = [data.shape[1] - 1]\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 2\n",
    "# Number of training points\n",
    "n_train = 1500\n",
    "# Grid size = grid_size x grid_size\n",
    "grid_size = 50\n",
    "\n",
    "print(\"We will learn a linear algorithm on \", n_train, \" training examples\")\n",
    "\n",
    "# Uncomment to get non-deterministic results\n",
    "np.random.seed(3395)\n",
    "# Randomly assign indices for the training and test set\n",
    "inds = np.arange(data.shape[0])\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "\n",
    "# Separate the dataset into the two sets\n",
    "train_set = data[train_inds,:]\n",
    "train_set = train_set[:,train_cols + target_ind]\n",
    "test_set = data[test_inds,:]\n",
    "test_set = test_set[:,train_cols + target_ind]\n",
    "\n",
    "# Separate the test set into inputs and labels\n",
    "test_inputs = test_set[:,:-1]\n",
    "test_labels = test_set[:,-1]\n",
    "\n",
    "mu = 0.00005\n",
    "model = KernelPerceptron(mu, kernel_polynomial)\n",
    "model.train(train_set)\n",
    "\n",
    "# Obtain its predictions\n",
    "t1 = time.clock()\n",
    "outputs = model.compute_predictions(test_inputs)\n",
    "\n",
    "t2 = time.clock()\n",
    "print('It took us ', t2-t1, ' seconds to calculate predictions on ', test_inputs.shape[0],' test points')\n",
    "\n",
    "# Majority vote (+1 since our classes are labeled from 1 to n)\n",
    "classes_pred = np.sign(outputs)\n",
    "\n",
    "# Run the tests\n",
    "err = 1.0 - np.mean(test_labels==classes_pred)\n",
    "print(\"The test error is of \", 100.0 * err,\"%\")\n",
    "\n",
    "if len(train_cols) == 2:\n",
    "    # Decision boundary\n",
    "    outputs = model.compute_predictions(train_set[:,:-1])\n",
    "    train_classes_pred = np.sign(outputs)\n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_classes_pred)\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=classes_pred)\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_set[:,-1])\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=test_labels)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Too many dimensions (', len(train_cols),') to plot the decision boundary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
